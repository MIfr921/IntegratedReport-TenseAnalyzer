import streamlit as st
import pdfplumber
import re
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import font_manager
from collections import Counter
from janome.tokenizer import Tokenizer
import os, requests

# ===== æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’å‹•çš„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCloudå¯¾å¿œï¼‰ =====
FONT_URL = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf"
FONT_PATH = "NotoSansCJKjp-Regular.otf"

if not os.path.exists(FONT_PATH):
    r = requests.get(FONT_URL)
    with open(FONT_PATH, "wb") as f:
        f.write(r.content)

plt.rcParams['font.family'] = font_manager.FontProperties(fname=FONT_PATH).get_name()
plt.rcParams['axes.unicode_minus'] = False

# ===== ãƒšãƒ¼ã‚¸è¨­å®š =====
st.set_page_config(page_title="çµ±åˆå ±å‘Šæ›¸PDFèªå°¾ãƒ»æ™‚åˆ¶åˆ†æã‚¢ãƒ—ãƒª", layout="wide")
st.title("ğŸ“„ çµ±åˆå ±å‘Šæ›¸PDFèªå°¾ãƒ»æ™‚åˆ¶åˆ†æã‚¢ãƒ—ãƒª")
st.write("ä¼æ¥­ã®çµ±åˆå ±å‘Šæ›¸PDFã‹ã‚‰æ–‡æœ«èªå°¾ã¨æ™‚åˆ¶ï¼ˆéå»å½¢ãƒ»ç¾åœ¨å½¢ï¼‰ã‚’åˆ†æã—ã€æ–‡ä½“å‚¾å‘ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

# ===== PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
uploaded_file = st.file_uploader("åˆ†æã—ãŸã„çµ±åˆå ±å‘Šæ›¸PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["pdf"])

if uploaded_file is not None:
    with st.spinner("PDFã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        def extract_text_from_pdf(file):
            all_text = ""
            with pdfplumber.open(file) as pdf:
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        all_text += text + "\n"
            return all_text

        text = extract_text_from_pdf(uploaded_file)
        st.success("âœ… PDFãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
        st.write("ğŸ“– æŠ½å‡ºã•ã‚ŒãŸå†’é ­éƒ¨åˆ†ï¼š")
        st.code(text[:500] + "..." if len(text) > 500 else text)

    # --- æ–‡åˆ†å‰² ---
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    # --- æ™‚åˆ¶åˆ†é¡ ---
    def get_tense(sentence):
        if re.search(r'(ãŸ|ã ã£ãŸ|ã¾ã—ãŸ|ã§ã—ãŸ)[^ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]*$', sentence):
            return "éå»å½¢"
        else:
            return "ç¾åœ¨ãƒ»æœªæ¥å½¢"

    data = [{"æ–‡": s, "åŒºåˆ†": get_tense(s)} for s in sentences]
    df = pd.DataFrame(data)

    # ===== ğŸ¥§ æ™‚åˆ¶ã®å‰²åˆ =====
    st.subheader("ğŸ“ˆ æ™‚åˆ¶ã®å‰²åˆï¼ˆæ–‡æ•°ãƒ™ãƒ¼ã‚¹ï¼‰")
    tense_counts = df["åŒºåˆ†"].value_counts()
    fig_ratio, ax_ratio = plt.subplots(figsize=(5,5))
    ax_ratio.pie(
        tense_counts,
        labels=tense_counts.index,
        autopct="%1.1f%%",
        startangle=90,
        colors=["cornflowerblue", "orange"]
    )
    ax_ratio.axis("equal")
    ax_ratio.set_title("éå»å½¢ vs ç¾åœ¨ãƒ»æœªæ¥å½¢ ã®å‰²åˆ")
    st.pyplot(fig_ratio)

    st.dataframe(pd.DataFrame(tense_counts).rename(columns={"åŒºåˆ†":"æ–‡æ•°"}))

    # --- æ–‡æœ«èªå°¾æŠ½å‡º ---
    def extract_sentence_ending(s):
        match = re.search(r'(ã§ã‚ã‚‹|ã¨ãªã‚Šã¾ã™|ã«ãªã‚Šã¾ã™|ã„ãŸã—ã¾ã™|ã§ã—ãŸ|ã ã£ãŸ|ã¾ã™|ã¾ã—ãŸ|ã§ã™|ã )$', s)
        return match.group(1) if match else None

    endings = [extract_sentence_ending(s) for s in sentences if extract_sentence_ending(s)]
    ending_counts = Counter(endings)
    df_endings = pd.DataFrame(ending_counts.items(), columns=["èªå°¾", "å‡ºç¾å›æ•°"]).sort_values("å‡ºç¾å›æ•°", ascending=False)

    # --- æ£’ã‚°ãƒ©ãƒ• ---
    st.subheader("ğŸ“Š æ–‡æœ«èªå°¾ã®å‡ºç¾é »åº¦")
    st.dataframe(df_endings, use_container_width=True)

    fig1, ax1 = plt.subplots(figsize=(6,4))
    ax1.barh(df_endings["èªå°¾"], df_endings["å‡ºç¾å›æ•°"], color="steelblue")
    ax1.invert_yaxis()
    ax1.set_title("æ–‡æœ«èªå°¾ã®å‡ºç¾é »åº¦", fontsize=14)
    ax1.set_xlabel("å‡ºç¾å›æ•°")
    st.pyplot(fig1)

    # --- æ™‚åˆ¶åˆ¥é »å‡ºèª ---
    st.subheader("ğŸ•° æ™‚åˆ¶åˆ¥é »å‡ºèªï¼ˆä¸Šä½20èªï¼‰")
    tokenizer = Tokenizer()
    def extract_words(text):
        words = []
        for token in tokenizer.tokenize(text):
            pos = token.part_of_speech.split(',')[0]
            if pos in ["åè©", "å‹•è©", "å½¢å®¹è©"]:
                words.append(token.base_form)
        return words

    word_freq = {}
    for label, group in df.groupby("åŒºåˆ†"):
        words = []
        for sentence in group["æ–‡"]:
            words.extend(extract_words(sentence))
        word_freq[label] = Counter(words).most_common(20)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### ğŸ”µ éå»å½¢ã§é »å‡ºã—ãŸå˜èª")
        past_df = pd.DataFrame(word_freq.get("éå»å½¢", []), columns=["å˜èª", "å‡ºç¾å›æ•°"])
        st.dataframe(past_df)
        if not past_df.empty:
            fig2, ax2 = plt.subplots(figsize=(6,4))
            ax2.barh(past_df["å˜èª"], past_df["å‡ºç¾å›æ•°"], color="cornflowerblue")
            ax2.invert_yaxis()
            ax2.set_title("éå»å½¢ï¼šé »å‡ºå˜èª")
            st.pyplot(fig2)

    with col2:
        st.markdown("#### ğŸŸ  ç¾åœ¨ãƒ»æœªæ¥å½¢ã§é »å‡ºã—ãŸå˜èª")
        future_df = pd.DataFrame(word_freq.get("ç¾åœ¨ãƒ»æœªæ¥å½¢", []), columns=["å˜èª", "å‡ºç¾å›æ•°"])
        st.dataframe(future_df)
        if not future_df.empty:
            fig3, ax3 = plt.subplots(figsize=(6,4))
            ax3.barh(future_df["å˜èª"], future_df["å‡ºç¾å›æ•°"], color="orange")
            ax3.invert_yaxis()
            ax3.set_title("ç¾åœ¨ãƒ»æœªæ¥å½¢ï¼šé »å‡ºå˜èª")
            st.pyplot(fig3)

    csv = df_endings.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ æ–‡æœ«èªå°¾é›†è¨ˆçµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="èªå°¾é›†è¨ˆçµæœ.csv", mime="text/csv")

else:
    st.info("ğŸ‘† ä¸Šã®ãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰çµ±åˆå ±å‘Šæ›¸PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
